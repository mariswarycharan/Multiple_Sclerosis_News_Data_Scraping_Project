{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for Multiple pages\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def check_response_status(url):\n",
    "\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:\n",
    "            print(f\"Success: Status code {response.status_code}\")\n",
    "        else:\n",
    "            print(f\"Failed: Status code {response.status_code}\")\n",
    "        return response\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return None\n",
    "\n",
    "def get_news_release_links(base_url, pages=1):\n",
    "    links = []\n",
    "    for page in range(1, pages + 1):\n",
    "        url = f\"{base_url}&page={page}\"\n",
    "        response = check_response_status(url)\n",
    "\n",
    "        if response is None or response.status_code != 200:\n",
    "            print(f\"Skipping page {page} due to error.\")\n",
    "            continue\n",
    "\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        for link in soup.find_all('a', class_='news-release', href=True):\n",
    "            full_url = f\"https://www.prnewswire.com{link['href']}\"  \n",
    "            links.append(full_url)\n",
    "\n",
    "    return links\n",
    "\n",
    "def extract_news_content(url):\n",
    "    response = check_response_status(url)\n",
    "\n",
    "    if response is None or response.status_code != 200:\n",
    "        print(f\"Skipping URL due to error: {url}\")\n",
    "        return None\n",
    "\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Extract title\n",
    "    headline_section = soup.find('div', class_='row detail-headline')\n",
    "    if headline_section:\n",
    "        title_element = headline_section.find('h1')\n",
    "        title_text = title_element.get_text(strip=True) if title_element else \"No title found\"\n",
    "    else:\n",
    "        title_text = \"No title found\"\n",
    "\n",
    "    # Extract date\n",
    "    date_element = soup.find('p', class_='mb-no')\n",
    "    date_text = date_element.get_text(strip=True) if date_element else \"No date found\"\n",
    "\n",
    "    # Extract \"News provided by\"\n",
    "    news_provider_section = soup.find('div', class_='col-lg-8 col-md-8 col-sm-7 swaping-class-left')\n",
    "    if news_provider_section:\n",
    "        provider_element = news_provider_section.find('strong')\n",
    "        news_provided_by = provider_element.get_text(strip=True) if provider_element else \"No provider found\"\n",
    "    else:\n",
    "        news_provided_by = \"No provider found\"\n",
    "\n",
    "    # Extract body\n",
    "    body_section = soup.find('div', class_='col-lg-10 col-lg-offset-1')\n",
    "    body_section_2 = soup.find('div', class_='col-sm-10 col-sm-offset-1')\n",
    "    # Initialize an empty string to hold the body text\n",
    "    body_text = \"\"\n",
    "\n",
    "    # If body_section exists, add its text to body_text\n",
    "    if body_section:\n",
    "        body_text += body_section.get_text(separator='\\n', strip=True)\n",
    "\n",
    "    # If body_section_2 exists, concatenate its text to body_text\n",
    "    if body_section_2:\n",
    "        body_text += \"\\n\" + body_section_2.get_text(separator='\\n', strip=True)\n",
    "\n",
    "    # If neither section exists, set a default message\n",
    "    if not body_text:\n",
    "        body_text = \"No body content found\"\n",
    "\n",
    "    return {\n",
    "        \"url\": url,\n",
    "        \"title\": title_text,\n",
    "        \"date\": date_text,\n",
    "        \"news_provided_by\": news_provided_by,\n",
    "        \"body\": body_text\n",
    "    }\n",
    "\n",
    "\n",
    "base_url = \"https://www.prnewswire.com/search/news/?keyword=MULTIPLE%20SCLEROSIS\"\n",
    "\n",
    "\n",
    "news_release_links = get_news_release_links(base_url, pages=16)\n",
    "\n",
    "\n",
    "news_contents = []\n",
    "for link in news_release_links:\n",
    "    content = extract_news_content(link)\n",
    "    if content:\n",
    "        news_contents.append(content)\n",
    "\n",
    "# Save the extracted data to an Excel file\n",
    "df = pd.DataFrame(news_contents) \n",
    "excel_file = \"www.prnewswire.com_MULTIPLE SCLEROSIS.xlsx\"\n",
    "df.to_excel(excel_file, index=False)\n",
    "\n",
    "print(f\"Data has been saved to {excel_file}.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
